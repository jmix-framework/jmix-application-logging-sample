:sample-project: jmix-$PROJECT_NAME-sample

= Application Logging in Jmix

To monitor and understand the behavior of a running Jmix application, effective logging is essential. The Java ecosystem provides mature tools and methods for implementing application logging. This guide will demonstrate how to leverage this ecosystem within a Jmix application.

[[requirements]]
== Requirements

If you want to implement this guide step by step, you will need the following:

1. xref:ROOT:setup.adoc[Setup Jmix Studio]
2. Download the sample project. You can **download the completed sample project**, which includes all the examples used in this guide. This allows you to explore the finished implementation and experiment with the functionality right away.
* https://github.com/jmix-framework/{sample-project}/archive/refs/heads/main.zip[Download^] and unzip the source repository
* or clone it using git:
`git clone https://github.com/jmix-framework/{sample-project}.git`

Alternatively, you can **start with the base Petclinic project** and follow the step-by-step instructions in this guide to implement the features yourself: https://github.com/jmix-framework/jmix-petclinic-2[Jmix Petclinic] and follow along to add the functionality step-by-step.

[[what-we-are-going-to-build]]
== What We are Going to Build

In this guide, we’ll enhance the https://github.com/jmix-framework/jmix-petclinic-2[Jmix Petclinic^] example to showcase logging integration. You’ll learn how to configure logging output for standard Java output streams, Docker logs, and external monitoring tools like Elasticsearch for comprehensive, external log management.


The application includes the following logging related features:

- **Logger Injection**: Learn how to inject and use a logger within Spring Beans, allowing structured logging throughout application services.
- **MDC Context Logging**: Discover how to set up MDC (Mapped Diagnostic Context) to include contextual information like a pet ID, enhancing traceability within logging.
- **Centralized Logging Solutions**: Integrate with centralized logging systems, such as Elasticsearch, to manage and monitor logs effectively outside the application.


[[final-application]]
=== Final Application

video::zTYx_KSeMzY[youtube,width=1280,height=600]

[[why-application-loggig-is-essential]]
==== Why Application Logging is Essential

Once an application is deployed, logging becomes one of the primary ways to gain insight into its operational state. Logs offer visibility into the application’s flow, making them essential for understanding behavior in production. They allow developers to identify and diagnose issues, capturing details about unexpected events or failures, and providing context that helps with debugging without direct access to the code.

In production, observability extends beyond logging to include metrics and custom business events, which together create a fuller picture of an application’s performance and user interactions. Metrics track quantitative data (like response times or resource usage), while business events capture high-level actions within the system. Together with logging, these elements support monitoring and troubleshooting effectively.

In this guide, we’ll focus on logging as a foundational aspect of observability, exploring practical configurations and tools for effective monitoring in Jmix applications.

In the example below, two log statements are generated. When a `Pet` instance is successfully stored, an info log captures the pet’s details. If storing the Pet fails, an error log records information about the failure, aiding in troubleshooting and monitoring.

.PetService.java
[source,java,indent=0]
----
include::example$/src/main/java/io/jmix/petclinic/service/PetService.java[tags=pet-service-logging]
----

Log messages are typically written either to standard output in the console or to a text file, allowing administrators and developers to access them for review. In production environments, where direct debugging is limited, logging becomes an essential diagnostic tool for understanding application behavior.

For example:

[source,shell]
----
2024-10-11T12:33:35.664+02:00  INFO 80773 --- [nio-8080-exec-3] io.jmix.petclinic.service.PetService     : Pet io.jmix.petclinic.entity.pet.Pet-acf0839d-d96c-a286-9b0d-452548ebb70a [detached] was saved correctly
----

This log entry captures a successful save action, providing useful context about the `Pet` entity and where the action took place.

[[the-java-logging-ecosystem]]
== The Java Logging Ecosystem

Logging has always played a significant role in the Java ecosystem, just as in many other platforms. Several APIs and libraries provide mechanisms for logging in Java applications, helping separate the technical tasks of logging data from the action of "writing a log message."

Jmix, like Spring Boot, uses Logback as its standard logging library. This choice provides stability, extensive configuration options, and seamless integration for developers working within the Spring Boot environment of Jmix applications. Spring Boot makes it straightforward to set up and customize logging configurations. While alternatives exist, Logback’s stability and integration with Spring Boot make it the recommended choice for Jmix applications.

In the Java ecosystem, a widely adopted combination includes:

- *Slf4J*: An abstract API that provides methods for logging at various severity levels (`DEBUG`, `INFO`, `WARN`, etc.). Slf4J allows developers to interact with logging without being tied to a specific logging library.

- *Logback*: The logging library that implements Slf4J's API, handling the actual writing of log events. Logback offers a range of configurations, including multiple appenders that direct log messages to files, consoles, or centralized logging systems.

This guide will focus on leveraging this combination, with examples of configuring Logback in a Jmix application.

For more details on setting up and customizing logging in Spring Boot, including Logback configurations and integration with Slf4J, refer to the official Spring Boot documentation:

https://docs.spring.io/spring-boot/reference/features/logging.html[Spring Boot Logging Documentation]

[[how-to-log-in-a-jmix-application]]
== How to Log in a Jmix Application

Using logging libraries in a Jmix application is straightforward, much like in any other Java application. Jmix, by default, is configured with Slf4J and Logback, which are also used internally for framework logging.

To log information within a class, let’s revisit the example above to see exactly how to set up logging in a Jmix or Spring-based class:

.PetService.java
[source,java,indent=0]
----
include::example$/src/main/java/io/jmix/petclinic/service/PetService.java[tags=logging-imports;pet-service-logger;pet-service-logging]
----
<1> Defines the logger for this class via Slf4J.
<2> Logs an info message upon successful save, with variable data passed as placeholders ({}).
<3> Logs an error message if saving fails, including exception details.

[[logging-levels]]
=== Logging Levels

In logging, messages can vary greatly in importance, from routine information to critical errors. To help distinguish these messages, logging libraries offer log levels, indicating the urgency or criticality of each message. Slf4J categorizes log levels in increasing severity:

- `TRACE`
- `DEBUG`
- `INFO`
- `WARN`
- `ERROR`
- `FATAL`

These categories serve as a general framework, and while they provide helpful guidelines, there are no strict standards dictating when each level should be used. Developers are encouraged to apply these levels based on the specific context of their application.

For more information, see the official Slf4J documentation on https://www.slf4j.org/api/org/apache/log4j/Level.html[logging levels].

[[adjusting-logging-configuration]]
== Adjusting Logging Configuration

In Jmix applications, logging configuration is managed through Spring Boot’s logging system. While Jmix does not include a `logback-spring.xml` file by default, you can create one in the `src/main/resources` directory to define appenders, log levels, and logging destinations.

A simple example configuration might look like this:

.logback-spring.xml
[source,xml,indent=0]
----
include::example$/src/main/resources/logback-spring.xml[tags=]
----

=== Changing Logging Levels

In Spring Boot, including Jmix applications, you can easily adjust logging levels through the `application.properties` file. By default, Jmix uses Logback as its logging framework, and many logging levels are pre-configured, as seen below. These levels can be modified to increase or decrease verbosity depending on the needs of the environment (development vs production).

.application.properties
[source,properties,indent=0]
----
include::example$/src/main/resources/application.properties[tags=logging-level-configuration]
----

This configuration defines different logging levels for various components of the Jmix application and the underlying libraries like `EclipseLink` and `Liquibase`. For example:
- The logging level for `io.jmix` is set to `INFO`, which means only important informational messages, warnings and errors will be logged.
- The level for `liquibase` is set to `WARN`, so only warning and error messages are logged, reducing verbosity.

To change a logging level, simply update the corresponding line in `application.properties`, and the new level will take effect after restarting the application. This flexibility allows you to tailor the logging output for different environments, such as more detailed logging in development and more minimal logs in production.

You can configure logging levels for any package or class that is not already defined by simply adding `logging.level.{packageName}` to the `application.properties` file. You can also specify the logging level for individual classes directly.

For example:
[source,properties,indent=0]
----
logging.level.io.jmix.petclinic.service = debug
----

This configuration sets all classes in the `service` package of the sample application to log messages at the `DEBUG` level and above.

For more information on logging levels and configuration options, refer to the official Spring Boot documentation:
https://docs.spring.io/spring-boot/reference/features/logging.html#features.logging.log-levels[Spring Boot Logging Levels Documentation^].

[[dynamic-logging-levels-environment]]
=== Changing Logging Levels with Environment Variables

In addition to configuring logging levels in `application.properties`, Spring Boot allows you to modify logging levels dynamically using environment variables. This approach is especially useful in containerized environments (e.g., Docker or Kubernetes), where configuration changes can be made at deployment time without modifying the source code.

To change a logging level using an environment variable, you can set the `LOGGING_LEVEL` prefix followed by the package or class name:

.Example: Setting logging level via environment variable in Docker
[source,bash,indent=0]
----
$ docker run -e LOGGING_LEVEL_IO_JMIX_PETCLINIC_SERVICE=DEBUG my-jmix-app
----

In this example, the logging level for the `io.jmix.petclinic.service` package is set to `DEBUG` at deployment time. You can also combine this with deployment-specific configurations to ensure that different logging levels are applied based on the environment.

.Spring Boot environment variable format
[source,bash,indent=0]
----
LOGGING_LEVEL_{PACKAGE_NAME}={LEVEL}
----

This dynamic approach is useful for scenarios where you want to adjust the logging verbosity without changing the application’s configuration files. It supports more flexibility, especially when managing multiple deployments or environments.

[[logging-sql-statements]]
=== Logging SQL Statements in Jmix

It is often useful to enable SQL logging, especially when trying to understand database interactions. This is particularly helpful in situations such as debugging performance issues, identifying inefficiencies in query execution, or verifying correct database operations.

By default, Jmix sets the logging level for SQL logging to `INFO` in the `application.properties` file. However, you can easily adjust this to `debug` by updating the logging level for `eclipselink.logging.sql`, allowing you to see detailed SQL queries and parameter bindings in the logs.

.application.properties
[source,properties,indent=0]
----
include::example$/src/main/resources/application.properties[tags=logging-level-eclipselink-sql]
----

With this configuration, SQL statements are generated and logged, allowing you to closely inspect the actual database queries being executed. This helps verify that the correct queries are being sent to the database, ensuring that your application is functioning as expected. It also provides insight into how data is fetched and processed.

This is especially useful when diagnosing performance issues. For example, if you see multiple queries being executed for what seems like a single operation, it could indicate an N+1 query issue or improper use of lazy loading. In such cases, optimization may be needed by adjusting the fetch strategy to avoid unnecessary queries.

Here’s an example where multiple queries are executed to load a `Pet`, its `Owner`, and `Pet Type`. The fact that three separate queries are executed suggests that optimization through the correct use of a fetch plan could improve performance:

[source,log,indent=0]
----
2024-10-23T07:29:53.664+02:00 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 712486116> SELECT ID, BIRTHDATE, CREATED_BY, CREATED_DATE, DELETED_BY, DELETED_DATE, IDENTIFICATION_NUMBER, LAST_MODIFIED_BY, LAST_MODIFIED_DATE, NAME, VERSION, OWNER_ID, TYPE_ID FROM PETCLINIC_PET WHERE ((ID = ?) AND (0=0))
	bind => [098b43a9-e9a2-e6c7-be5d-10f650e3849b]
2024-10-23T07:29:53.665+02:00 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 712486116> [0 ms] spent
2024-10-23T07:29:53.666+02:00 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 2009192390> SELECT ID, ADDRESS, CITY, CREATED_BY, CREATED_DATE, DELETED_BY, DELETED_DATE, EMAIL, FIRST_NAME, LAST_MODIFIED_BY, LAST_MODIFIED_DATE, LAST_NAME, TELEPHONE, VERSION FROM PETCLINIC_OWNER WHERE ((ID = ?) AND (0=0))
	bind => [c3bb4197-4189-c26a-2aa9-35c0ebb9faa4]
2024-10-23T07:29:53.667+02:00 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 2009192390> [1 ms] spent
2024-10-23T07:29:53.668+02:00 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 662306096> SELECT ID, COLOR, CREATED_BY, CREATED_DATE, DELETED_BY, DELETED_DATE, LAST_MODIFIED_BY, LAST_MODIFIED_DATE, NAME, VERSION FROM PETCLINIC_PET_TYPE WHERE ((ID = ?) AND (0=0))
	bind => [1e2abb1f-5f77-865e-17fa-b67e85497523]
----

In this example, you can see that EclipseLink uses prepared statements to help preventing SQL injection attacks. The SQL queries are parameterized, and the actual parameter values are shown in the `bind` statement. This means the SQL query itself contains placeholders (`?`), and the real values are "bound" to those placeholders when the query is executed. As a result, the logged queries cannot be directly run in a database explorer without manually substituting the bound values into the `?` placeholders.

[[centralizing-context-information-in-logs]]
== Centralizing Context Information in Logs

As logging grows in an application, certain pieces of information—such as IDs or user details—tend to appear frequently in log messages. In the petclinic example, many log messages would likely contain the Pet ID to provide context efficiently. To avoid repeating this information in every log statement, we can centralize it.

_Logback_ provides a feature called _MDC_ (Mapped Diagnostic Context), which allows you to set specific values into a context that will automatically be included in every log message. This simplifies logging by ensuring that important context information is consistently logged without needing to be manually added to each log statement.
In the petclinic example, the `PetService` leverages the MDC context to track the Pet’s identification number before performing an update. The Pet ID is set in the MDC, and all log messages related to that operation automatically include the Pet ID, without needing to manually add it each time.

.PetService.java
[source,java,indent=0]
----
include::example$/src/main/java/io/jmix/petclinic/service/PetService.java[tags=logging-imports;pet-service-logger;pet-service-logging-mdc]
----
<1> The Pet's identification number is set into the MDC.
<2> A log message is recorded before updating the Pet.
<3> After the Pet is successfully updated, another log confirms the success.
<4> If an error occurs, an error log is created with exception details.
<5> The MDC is cleared at the end of the method to avoid context leakage.

Here is an example of the log output from the `PetService` with the Pet ID and user automatically included using MDC:

[source,log,indent=0]
----
2024-10-21T08:13:30.392+02:00 jmixUser:joy petId:088  INFO 30254 --- [nio-8080-exec-5] io.jmix.petclinic.service.PetService     : Updating Pet
2024-10-21T08:13:30.407+02:00 jmixUser:joy petId:088  INFO 30254 --- [nio-8080-exec-5] io.jmix.petclinic.service.PetService     : Pet Update successfully
----

In this example, you can see that the **Pet ID** (`088`) and **jmixUser** (`joy`) are automatically included in each log message.

[TIP]
====
By default, Jmix automatically writes the current username into the MDC under the key `jmixUser`. This allows you to include the current user in log messages without needing to manually handle the MDC for the user. Simply use `%X{jmixUser}` in your logging pattern to include the current user in logs.
====

The MDC context in Logback allows you to set contextual data, like a Pet ID, at any point in your application (such as in a View-Controller), and this data will automatically propagate across method calls and class boundaries, like when interacting with a Service. Once set, the Pet ID is available to all subsequent log messages in that flow, ensuring that consistent information is logged without needing to pass the ID explicitly between classes. This feature simplifies logging across different layers of your application.

Another advantage is that the MDC context is preserved when calling deeper framework-level code. This means that when you invoke a framework method, like saving an entity with Jmix’s DataStore or even deeper with EclipseLink, the MDC context remains intact. To see this in action, let's activate the framework data storage logs and see how MDC values are also included there:

.application.properties
[source,properties,indent=0]
----
logging.level.eclipselink.logging.sql=debug
logging.level.io.jmix.core.datastore=debug
----

Here’s an example log output showing how the **Pet ID** (`087`) and **jmixUser** (`joy`) are included not only in the application-level log messages but also in the internal framework logs during the pet update operation:

[source,log,indent=0]
----
2024-10-23T07:29:53.659+02:00 jmixUser:joy petId:087  INFO 6166 --- [nio-8080-exec-8] io.jmix.petclinic.service.PetService     : Updating Pet
2024-10-23T07:29:53.660+02:00 jmixUser:joy petId:087 DEBUG 6166 --- [nio-8080-exec-8] i.jmix.core.datastore.AbstractDataStore  : save: store=main, entities to save: [io.jmix.petclinic.entity.pet.Pet-098b43a9-e9a2-e6c7-be5d-10f650e3849b [detached]], entities to remove: []
2024-10-23T07:29:53.675+02:00 jmixUser:joy petId:087 DEBUG 6166 --- [nio-8080-exec-8] eclipselink.logging.sql                  : <t 459632279, conn 1441249747> UPDATE PETCLINIC_PET SET BIRTHDATE = ?,  LAST_MODIFIED_DATE = ?, VERSION = ? WHERE ((ID = ?) AND (VERSION = ?))
    bind => [1997-08-16, 2024-10-23T07:29:53.670+02:00, 3, 098b43a9-e9a2-e6c7-be5d-10f650e3849b, 2]
2024-10-23T07:29:53.683+02:00 jmixUser:joy petId:087  INFO 6166 --- [nio-8080-exec-8] io.jmix.petclinic.service.PetService     : Pet Update successfully
----

In this log output, the **Pet ID** and **jmixUser** values are consistently included not only in your custom application logs (`io.jmix.petclinic.service.PetService`) but also in the framework's internal logs, such as those from the Jmix `AbstractDataStore` and EclipseLink SQL operations. This allows you to easily link SQL queries or other framework-level operations to their functional context, even when the context might not be obvious from the logs themselves.

[WARNING]
====
It is important to clear the MDC context after the operation is complete using `MDC.remove("petId")`. If not cleared, the MDC values will persist and could affect subsequent log messages that may be unrelated to the current operation.
====

For more details on custom logging configuration in Spring Boot, refer to the official https://docs.spring.io/spring-boot/reference/features/logging.html#features.logging.custom-log-configuration[Spring Boot Custom Logging Configuration^].

== Logging to a Centralized Logging Solution

Centralized logging offers significant advantages when dealing with large amounts of log data. Instead of storing logs locally on individual servers, centralized systems aggregate logs into a single, easily accessible platform. This provides:

- **Easy accessibility**: Logs can be accessed through a web interface, making them searchable and easier to explore. This enables real-time troubleshooting and monitoring without requiring direct access to the servers.
- **Collaboration**: Centralized logging allows team members to share and link logs, which can help in debugging or reviewing incidents together.
- **Correlating logs**: Logs from multiple services can be aggregated in one place, making it easier to correlate events across different systems or services.
- **Relevance in microservices architecture**: As distributed architectures like microservices have grown in popularity, centralized logging systems have become a critical tool for maintaining visibility across all services and correlating events.

There are many providers for centralized logging solutions, such as Datadog, New Relic, or self-hosted options. In this example, we will use the popular ELK Stack (Elasticsearch, Logstash, Kibana) to demonstrate how to integrate Jmix with a centralized logging system.

=== Setting up the ELK Stack with Docker

To set up the ELK Stack, we will use Docker to run Elasticsearch, Logstash, and Kibana. This setup will allow us to collect, store, and visualize logs in real-time. Start by creating a `docker-compose.yml` file in the root of your project and add the following configuration:

[source,yml,indent=0]
----
include::example$/src/../docker-compose.yml
----

This configuration does the following:
- **Elasticsearch** stores log data.
- **Logstash** collects logs from the Jmix application and forwards them to Elasticsearch.
- **Kibana** provides a web interface for visualizing and searching log data.

You can start these services with the following command:

[source,bash,indent=0]
----
$ docker-compose up
----

Once the services are running, Kibana will be accessible at `http://localhost:5601`, where you can explore and visualize logs in real-time.

=== Configuring the Jmix Application for Centralized Logging

Next, we will configure the Jmix application to send logs to Logstash, which will forward them to Elasticsearch. This involves two steps: adding the necessary dependencies and modifying the logging configuration.

First, add the following dependencies to your `build.gradle` file:

.build.gradle
[source,gradle,indent=0]
----

dependencies {

    // ...

    implementation 'net.logstash.logback:logstash-logback-encoder:8.0'
    implementation 'ch.qos.logback:logback-classic:1.5.6'
}
----

These dependencies include the Logstash encoder and Logback classic, allowing us to configure Logstash in our logging configuration.

Next, modify the `logback-spring.xml` file to include a Logstash appender, which will send logs to the Logstash service:

.logback-spring.xml
[source,xml,indent=0]
----
include::example$/src/main/resources/logback-spring.xml[]
----

With this setup, the `LogstashTcpSocketAppender` sends logs from the Jmix application to Logstash. This allows us to centralize and process logs through Elasticsearch and visualize them in Kibana.

=== Viewing Logs in Kibana

Once the ELK Stack is up and running, you can access Kibana at `http://localhost:5601/app/logs`. This web interface allows you to search, filter, and visualize logs sent from your Jmix application. Kibana provides a powerful interface for exploring log data, enabling you to drill down into specific events, correlate logs across services, and create dashboards for monitoring.

The MDC values are stored in the Elasticsearch index as dedicated fields, which makes it possible to easily search for them and display them as columns in Kibana. This allows you to filter logs by MDC values such as **Pet ID** or **jmixUser** and see them directly in the log view. As shown in the screenshot below, these fields appear alongside standard log data, making it easier to analyze logs based on the custom context from your application:

image::jmix-kibana-logs.png[Kibana Logs Visualization, link="images/jmix-kibana-logs.png"]

To learn more about using Kibana to search and analyze logs, refer to the official Kibana documentation:
https://www.elastic.co/guide/en/kibana/current/discover.html[Kibana Discover Documentation^].

With this setup, you can now efficiently monitor and analyze your application's logs in a centralized location, making it easier to troubleshoot, optimize, and collaborate on any issues that arise.

[[summary]]
== Summary

This guide demonstrated how effective logging can be implemented in a Jmix application using the Java ecosystem. We explored basic logging concepts, such as injecting loggers into services, and advanced features like MDC (Mapped Diagnostic Context) to include contextual information, such as a Pet ID, across log messages automatically. This helps in maintaining a consistent context across different layers of the application, without manually passing identifiers.

We also looked at how logging levels can be customized for different environments, either through configuration files or dynamic environment variables. Additionally, we touched upon centralized logging solutions, like Elasticsearch, for managing and analyzing logs externally.

Logging is essential for observability and debugging in production environments. Properly configured logging ensures that administrators can track down issues without direct access to the running application, making it a core aspect of application maintenance and monitoring.

[[further-information]]
=== Further Information
* https://docs.spring.io/spring-boot/reference/features/logging.html[Spring Boot Logging Documentation^]